{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aac1d82",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27767443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#road file corpus\n",
    "df_corpus = pd.read_csv('[PATH_OF_FILE_df_cospus.csv]' )\n",
    "\n",
    "#df_corpus to list\n",
    "corpusList = df_corpus['corpus'].tolist()\n",
    "\n",
    "\n",
    "vocab = sorted(set(corpusList))\n",
    "print('Corpus length (in words):', len(corpusList))\n",
    "print('Unique words in corpus: {}'.format(len(vocab)))\n",
    "word2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2words = np.array(vocab)\n",
    "word_as_int = np.array([word2idx[c] for c in corpusList])\n",
    "\n",
    "\n",
    "# The maximum length sentence we want for a single input in words\n",
    "seqLength = 8\n",
    "examples_per_epoch = len(corpusList)//(seqLength + 1) # number of seqLength+1 sequences in the corpus\n",
    "\n",
    "# Create training / targets batches\n",
    "wordDataset = tf.data.Dataset.from_tensor_slices(word_as_int)\n",
    "print(wordDataset)\n",
    "sequencesOfWords = wordDataset.batch(seqLength + 1, drop_remainder=True) # generating batches of 8 words each, typically converting list of words (sequence) to string\n",
    "print(sequencesOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk): # This is where right shift happens\n",
    "  input_text = chunk[:-1]\n",
    "  print(input_text)\n",
    "  target_text = chunk[1:]\n",
    "  print(target_text)\n",
    "  return input_text, target_text # returns training and target sequence for each batch\n",
    "\n",
    "dataset = sequencesOfWords.map(split_input_target) # dataset now contains a training and a target sequence for each 8 word slice of the corpus\n",
    "\n",
    "\n",
    "#define BATCH_SIZE and BUFFER_SIZE\n",
    "BATCH_SIZE = 64 # each batch contains 64 sequences. Each sequence contains 8 words (seqLength)\n",
    "BUFFER_SIZE = 100 # Number of batches that will be processed concurrently\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Length of the vocabulary in words\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "# Number of GRU units\n",
    "rnn_units = 1024\n",
    "\n",
    "def createModel(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = createModel(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
    "\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") \n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "#Train Model\n",
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = createModel(len(vocab), embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "def generateLyrics(model, startString, temp,num_generate):\n",
    "  print(\"---- Generating lyrics starting with '\" + startString + \"' ----\")\n",
    "  # Number of words to generate\n",
    "  num_generate = num_generate\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  start_string_list =  [w for w in startString.split(' ')]\n",
    "  input_eval = [word2idx[s] for s in start_string_list]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # temp represent how 'conservative' the predictions are. \n",
    "      # Lower temp leads to more predictable (or correct) lyrics\n",
    "      predictions = predictions / temp \n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      text_generated.append(' ' + idx2words[predicted_id])\n",
    "\n",
    "  return (startString + ''.join(text_generated))\n",
    "\n",
    "\n",
    "#save trained model for future use (so we do not have to train it every time we want to generate text)\n",
    "model.save('model_generate_song.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fde59e",
   "metadata": {},
   "source": [
    "# Generate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3709a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7326b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (in words): 314907\n",
      "Unique words in corpus: 7125\n"
     ]
    }
   ],
   "source": [
    "#road file corpus\n",
    "# df_corpus = pd.read_csv('[PATH_OF_FILE_df_cospus.csv]' )\n",
    "df_corpus = pd.read_csv('dataset_corpus_lyrics.csv')\n",
    "\n",
    "#df_corpus to list\n",
    "corpusList = df_corpus['corpus'].tolist()\n",
    "\n",
    "\n",
    "vocab = sorted(set(corpusList))\n",
    "print('Corpus length (in words):', len(corpusList))\n",
    "print('Unique words in corpus: {}'.format(len(vocab)))\n",
    "word2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2words = np.array(vocab)\n",
    "word_as_int = np.array([word2idx[c] for c in corpusList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b2863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLyrics(model, startString, temp,num_generate):\n",
    "  print(\"---- Generating lyrics starting with '\" + startString + \"' ----\")\n",
    "  # Number of words to generate\n",
    "  num_generate = num_generate\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  start_string_list =  [w for w in startString.split(' ')]\n",
    "  input_eval = [word2idx[s] for s in start_string_list]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # temp represent how 'conservative' the predictions are. \n",
    "      # Lower temp leads to more predictable (or correct) lyrics\n",
    "      predictions = predictions / temp \n",
    "      # print(predictions)\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      # print(predicted_id)\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      # print(input_eval)\n",
    "\n",
    "      text_generated.append(' ' + idx2words[predicted_id])\n",
    "      \n",
    "\n",
    "  return (startString + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dba7873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# load Model\n",
    "from keras.models import load_model\n",
    "# model_w = load_model('[PATH_OF_MODEL_model_generate_song.h5]')\n",
    "model_w = load_model('model_generate_song.h5')\n",
    "#save trained model for future use (so we do not have to train it every time we want to generate text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c325d66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Generating lyrics starting with 'คิดถึง' ----\n",
      "คิดถึง ทีไร แค่นี้ เอง ก็ รู้ มัน อยู่ ใกล้ ใจ เธอ เป็น ของ ฉัน รู้ ว่า มัน ไม่ ดี กับ เธอ แม้ เรา มี เรา กัน ทุกอย่าง แค่ เพียง วูบ เดียว มา แค่ เพียง ชั่วคราว ผ่าน เข้ามา แล้ว ทำ ฉัน ต้อง เสีย ใจมา แค่ เพียง วูบ เดียว มา วูบ เดียว แค่ จาก ผ่าน เข้ามา แล้ว ทำให้ น้ำตา ริน ไหล ไม่ รู้ ว่า นาน เท่าไหร่ ที่จะ หลุดพ้น กับ ความรัก ที่ ทำให้ หมองหม่น เหลือเกิน แต่ ผ่าน มา เพียง วูบ เดียว มา แค่ เพียง ชั่วคราว ผ่าน เข้ามา แล้ว ทำ ฉัน ต้อง เสีย ใจมา แค่ เพียง วูบ เดียว มา วูบ เดียว แค่ จาก ผ่าน เข้ามา\n"
     ]
    }
   ],
   "source": [
    "print(generateLyrics(model_w, startString=\"คิดถึง\", temp=0.6,num_generate = 100))\n",
    "\n",
    "# while (True):\n",
    "#   print('Enter start string:')\n",
    "#   input_str = input().lower().strip()\n",
    "#   temp=0.6\n",
    "#   num_generate = 100\n",
    "#   print(generateLyrics(model_w, startString=input_str, temp=temp,num_generate=num_generate))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275f3dd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generateLyrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d5c10bb449a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerateLyrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartString\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"พ่อ\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_generate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'generateLyrics' is not defined"
     ]
    }
   ],
   "source": [
    "print(generateLyrics(model_w, startString=\"พ่อ\", temp=0.6,num_generate = 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91646b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Generating lyrics starting with 'แม่' ----\n",
      "แม่ ง นี่ ของจริง มัน ไม่ ใช่ อยาก จะ หนี ไป กับ จานบิน อยู่ บน โลก ใบ นี้ เปลี่ยน ให้ เธอ ได้ เห็น ทั้งที่ เธอ ยัง เหมือนเดิม อีก เท่าไหร่ เรื่อง เรา ไม่ ไป ไหน แม้กระทั่ง ตอนนี้ เขา ยังอยู่ ตรงนั้น ใน หัวใจ แม้ จะ นาน เท่าไหร่ ยัง รัก เธอ ตื่น จาก ภาพลวงตา แต่ ไม่ มี เธอ เคียง กาย เหมือน เป็น ดั่ง ความฝัน อาจ เป็น ดั่ง เช่นเคย ฉัน ยืน อยู่ ตรงนี้ ที่ หัวใจ ต้องการ สุดท้าย มัน กลายเป็น หลุมดำ แล้ว ฉัน คง ทำได้ แค่ เพียง มองดู เธอ ค่อยๆ ทรมาน ต่อไป เป็น กำลังใจ ให้ เธอ รู้ ว่า โอ้ เธอ นะ คนดี ได้ โปรด อย่า จาก\n"
     ]
    }
   ],
   "source": [
    "print(generateLyrics(model_w, startString=\"แม่\", temp=0.6,num_generate = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76af78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
